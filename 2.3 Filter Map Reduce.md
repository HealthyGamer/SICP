# Data Manipulation - Filter, Map, Reduce

Filter, map, and reduce functions are foundational in functional processing and data engineering. However, understanding them and how they work is helpful in other programming styles since they help organize data manipulation functions and make testing easier.

## Filter Function

The Filter function is to selectively process and refine a data set or collection by applying specific criteria or conditions. This allows devs to create a new data structure containing only those elements that satisfy the given criteria without modifying the original data set.

### Filter Example

```JavaScript
const numbers = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10];

function isOdd(number) {
  return number % 2 !== 0;
}

const oddNumbers = numbers.filter(isOdd);

console.log(oddNumbers); // Output: [1, 3, 5, 7, 9]
```

## Map Function

The Map function applies a specific transformation or operation to each element in a collection and creates a new data structure with the transformed elements without modifying the original data set. It is advantageous when a consistent transformation needs to be applied to all elements in a data set. For example, developers might use the Map function to convert temperatures from Celsius to Fahrenheit, adjust currency values based on exchange rates, or apply mathematical operations to numerical data.

### Map Example

```Javascript
const celsiusTemperatures = [0, 10, 20, 30, 40];

function celsiusToFahrenheit(celsius) {
  return (celsius * 9 / 5) + 32;
}

const fahrenheitTemperatures = celsiusTemperatures.map(celsiusToFahrenheit);

console.log(fahrenheitTemperatures); // Output: [32, 50, 68, 86, 104]
```

## Reduce Function

The Reduce function aggregates or summarizes elements in a collection by iteratively applying a specified binary function or operation.

Examples include:

- Calculating the sum or average of numerical values.
- Determining the minimum or maximum element.
- Concatenating strings.
- Combining elements to create a custom data structure.

### Reduce Examples

```Javascript
// Averaging values: Calculate the average of an array of numbers.

const numbers = [1, 2, 3, 4, 5];
const average = numbers.reduce((accumulator, currentValue, index, array) => {
  accumulator += currentValue;
  return index === array.length - 1 ? accumulator / array.length : accumulator;
}, 0);

// Flattening arrays: Flatten a multi-dimensional array into a single-dimensional array.

const nestedArray = [[1, 2], [3, 4], [5, 6]];
const flattenedArray = nestedArray.reduce((accumulator, currentValue) => {
  return accumulator.concat(currentValue);
}, []);

// Grouping data: Group elements in an array based on a specific property or condition.

const people = [
  { name: 'Alice', age: 30 },
  { name: 'Bob', age: 25 },
  { name: 'Charlie', age: 30 },
  { name: 'David', age: 25 },
];

const groupedByAge = people.reduce((accumulator, currentValue) => {
  const age = currentValue.age;
  if (!accumulator[age]) {
    accumulator[age] = [];
  }
  accumulator[age].push(currentValue);
  return accumulator;
}, {});
```

## Bringing Filter, Map, and Reduce Together Into Pipelines

Data processing pipelines refer to a sequence of steps or stages designed to transform, manipulate, and process data efficiently and systematically. Each stage in a pipeline takes an input (usually the previous stage's output), processes it, and produces a result that serves as input for the next stage. Pipelines are commonly used to handle large data sets and complex data manipulation tasks, enabling developers to break down processes into manageable, modular components.

Pipelines provide several advantages:
Modularity: Pipelines divide complex processes into smaller, manageable stages, promoting code organization and reusability.
Readability: Pipelined code is easier to understand and maintain, as each stage focuses on a specific task or transformation.
Scalability: Pipelines can be easily expanded or modified to accommodate new processing steps or data sources.
Performance: Pipelining allows for optimizations, parallel processing, and efficient resource utilization, improving the overall performance of data-driven applications.

Examples:
- F# to count Github tags in the Azure Docs repo: <https://github.com/perfectly-panda/fsharpfunctions/blob/9c32c793f0479c018d88a8ad3075448a55d2ab6d/fsharpfunctions/DataProcessing/DataProcessing_4.fs#L33>

- VueJS component using reduce to display data: <https://github.com/perfectly-panda/streamanalyticsdemo/blob/ae3baf10e8167513494878649833b8d534be4f67/WebApplication1/streamanalyticsdemo/src/components/TTC.vue#L35>

- TS data store that uses filter to create a premade query the returns only speicifc items: <https://github.com/perfectly-panda/DMAQuest/blob/5f8012584050a66fd5952812e64703a1e2114916/src/stores/AppStore.ts#L21>